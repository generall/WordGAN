import os
import re
from unittest import TestCase

import torch
from allennlp.data import Vocabulary

from word_gan.model.embedding_to_word import EmbeddingToWord
from word_gan.settings import DATA_DIR, TEST_DATA_DIR


class TestWordReconstruction(TestCase):
    def test_word_reconstruction(self):
        vect_pickup = torch.tensor([list(map(
            float,
            '0.566424 -0.565591 0.517313 0.037623 0.586895 0.044779 -0.720138 -0.537858 0.250331 -0.423935 -0.157338 '
            '-0.214741 0.387854 0.228247 0.085150 -0.475555 -0.015422 -0.271250 0.464336 -0.017159 -0.429801 '
            '-0.545923 -0.334343 -0.516006 0.445679 0.610725 -0.173949 -0.266765 0.079867 -0.176660 0.294395 '
            '-0.686128 0.283598 -0.003040 -0.008645 -0.768948 -0.071676 0.233558 -0.276534 -1.142045 0.129191 '
            '-0.249634 0.170905 -0.042946 0.623486 0.166066 -0.198974 0.299922 0.172866 0.189438 -0.108745 0.548275 '
            '0.355457 0.350647 -0.470752 0.906069 -0.651937 0.892769 -0.783814 -0.078849 0.487258 0.153465 0.035145 '
            '0.408864 0.644951 -0.154346 -0.156254 -0.542629 -0.350993 0.477882 -0.380021 0.008634 -0.363685 0.085334 '
            '-0.954892 -0.161051 0.371422 -0.495890 0.799783 -0.105321 0.218377 0.377046 0.011289 -0.210246 0.197650 '
            '-0.115478 0.441830 -0.176482 0.416498 0.422281 -0.405244 -0.265307 -0.376643 0.166720 0.411024 -0.191067 '
            '0.135167 -0.125041 0.019294 0.231942 0.305366 -0.138456 -0.189564 0.061148 -0.449309 -0.192363 -0.140159 '
            '-0.625253 0.615019 -0.227329 0.857283 -0.126009 -0.642773 -0.431166 0.110363 0.252014 0.014979 -0.037603 '
            '-0.164845 0.459573 0.256176 -0.031698 -0.466361 -0.328294 -0.082874 -0.159345 -0.319550 -0.220706 '
            '0.100949 0.287838 0.298843 0.159900 -0.224792 -0.462894 -0.458304 0.036835 -0.103715 0.155095 -0.776833 '
            '0.026841 -0.455342 -0.074697 0.335267 0.005424 -0.676021 0.616806 0.687426 -0.025902 1.124582 0.018572 '
            '-0.790011 0.490443 -0.338669 0.278164 -0.107623 -0.084515 0.405176 -0.371889 0.014508 -0.110513 0.044016 '
            '-0.387444 0.230442 -0.280058 -0.228754 0.109110 0.029395 -0.505746 0.720379 -0.056848 -0.322540 0.194373 '
            '0.372148 -0.614639 0.352231 -0.425695 -0.203663 -0.298967 -0.210720 -0.269119 0.658963 -0.342888 '
            '0.286617 0.177207 0.689011 0.226063 -0.250295 -0.266093 0.261173 0.324343 0.136720 -0.045497 -0.352979 '
            '0.468376 -0.649333 -0.587720 0.771907 0.770939 -0.648633 -0.010933 -0.429291 0.122087 0.152353 0.394160 '
            '0.377298 0.426542 -0.426247 -0.840873 -0.041260 0.298802 -0.297595 -0.010809 -0.065814 0.137421 0.459671 '
            '0.193688 0.227599 -0.070492 -0.961502 -0.474683 -0.827887 0.393916 0.119588 0.955511 0.280122 -0.447132 '
            '0.362305 -0.277316 -0.045116 -0.072885 0.369605 -0.279887 0.521049 0.080664 -0.275288 0.505968 0.695216 '
            '-0.059200 0.002953 0.052706 -0.365122 0.406670 0.223318 -0.596568 -0.419728 -0.470677 0.327472 0.776836 '
            '-0.161948 0.695562 -0.526699 0.226716 0.064962 0.212348 0.431807 0.247971 0.023881 -0.126436 0.022776 '
            '0.037019 -0.160534 -0.393696 -0.453217 0.708573 0.372897 0.522332 -0.205536 0.514713 0.185940 0.123264 '
            '0.740620 0.435590 -0.446383 0.062280 -0.607210 -0.090250 -0.490167 -0.063571 0.301825 0.242296 -0.581975 '
            '-0.036239 0.445417 -0.076736 0.166880 0.377787 -0.171617 0.454794 0.886838 -0.131912 -0.103644 -1.110502 '
            '0.048929 -0.157931 0.087857 -0.231842 0.089160 -0.126510 0.235636 0.016760'.split()))])

        namespace = 'target'
        vocab = Vocabulary.from_files(os.path.join(TEST_DATA_DIR, 'vocab'))

        print('vocab size:', vocab.get_vocab_size(namespace=namespace))

        weights_file = os.path.join(TEST_DATA_DIR, 'test_v2w_model.th')
        model_state = torch.load(weights_file, map_location=torch.device('cpu'))

        model: torch.nn.Module = EmbeddingToWord(embedding_size=300, words_count=vocab.get_vocab_size(namespace))

        model.load_state_dict(model_state)

        word_scores = model(vect_pickup)

        print("word_scores: ", word_scores)

        print("Shape:", word_scores.shape)

        word_score, word_idx = torch.max(word_scores, dim=1)

        print('Index: ', word_idx.item(), 'score', word_score.item())

        closest_word = vocab.get_token_from_index(word_idx.item(), namespace=namespace)

        print('word:', closest_word)

        self.assertEqual(closest_word, 'pickup')
